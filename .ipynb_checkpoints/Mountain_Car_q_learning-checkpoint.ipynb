{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer:\n",
    "    def __init__(self, env, RBF_n_components=500):\n",
    "        \n",
    "        # set the environment\n",
    "        self._env = env\n",
    "        # samples observations from the possible states in the environement\n",
    "        observation_examples = self.__gen_observation_samples(n=10000)\n",
    "        # generate a scaler (to scale data)\n",
    "        self._scaler = self._gen_scaler(samples=observation_examples)\n",
    "        # generate a featurizer\n",
    "        self._featurizer = self._gen_featurizer(n=RBF_n_components, samples=observation_examples) \n",
    "\n",
    "    def transform(self, observations):\n",
    "        # first scale the observation\n",
    "        scaled = self._scaler.transform(observations)\n",
    "        \n",
    "        # return featurized representation of the scaled observations\n",
    "        return self._featurizer.transform(scaled)\n",
    "        \n",
    "    def _gen_featurizer(self, n, samples):\n",
    "        # n: number of monte-carlo samples used to map features of the RBF kernel\n",
    "        # gamma: Parameter of RBF kernel: exp(-gamma * x^2)\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n",
    "        featurizer = FeatureUnion([\n",
    "            (\"rbf1\", RBFSampler(gamma=5.0, n_components=n)),\n",
    "            (\"rbf2\", RBFSampler(gamma=2.0, n_components=n)),\n",
    "            (\"rbf3\", RBFSampler(gamma=1.0, n_components=n)),\n",
    "            (\"rbf4\", RBFSampler(gamma=0.5, n_components=n)),\n",
    "        ])\n",
    "        # run this once to setup the featurizer\n",
    "        example_features = featurizer.fit_transform(self._scaler.transform(samples))\n",
    "        return featurizer\n",
    "    \n",
    "    def _gen_scaler(self, samples):\n",
    "        \"\"\"\n",
    "        Create a StandardScaler() based on the samples dataset\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(samples)\n",
    "\n",
    "        return scaler\n",
    "    \n",
    "    def __gen_observation_samples(self, n=10000):\n",
    "        \n",
    "        observation_examples = np.array([self._env.observation_space.sample() for x in range(n)])\n",
    "        return observation_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, env, feature_transformer, learning_rate):\n",
    "        self._env = env\n",
    "        self._models = []\n",
    "        self._feature_transformer = feature_transformer\n",
    "        self._models = self._initialize_models(n_models=env.action_space.n, learning_rate=learning_rate)\n",
    "        \n",
    "    def _initialize_models(self, n_models, learning_rate):\n",
    "        \"\"\"\n",
    "        Initialize models for each action in the env. space:\n",
    "        # Holds one SGDRegressor for each action\n",
    "        Use partial_fit() and initialize it with reset state and 0 Gain value.\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        for i in range(n_models):\n",
    "            model = SGDRegressor(learning_rate=learning_rate)\n",
    "            X = self._feature_transformer.transform([self._env.reset()]) # \n",
    "            y = [0] # TODO: What is this? Action?\n",
    "            model.partial_fit(X, y)\n",
    "            models.append(model)\n",
    "            \n",
    "        return models\n",
    "    \n",
    "    def predict(self, state):\n",
    "        X = self._feature_transformer.transform([state])\n",
    "        result = np.stack([m.predict(X) for m in self._models]).T\n",
    "        assert(len(result.shape) == 2)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def update(self, state, action, G):\n",
    "        \"\"\"\n",
    "        Update models given (s,a,G) tuple\n",
    "        \"\"\"\n",
    "        X = self._feature_transformer.transform([state])\n",
    "        assert(len(X.shape) == 2)\n",
    "        # TODO: This can be done be a dictionary lookup of action and its value.\n",
    "        self._models[action].partial_fit(X, [G])\n",
    "\n",
    "    def sample_action(self, state, eps):\n",
    "        # Action based on epsilon-greedy algorithm\n",
    "        if np.random.random() < eps:\n",
    "            return self._env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.predict(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(model, eps, gamma=0.9):\n",
    "    observation = model._env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    iters = 0\n",
    "    while not done and iters < MAX_ITERS:\n",
    "        action = model.sample_action(observation, eps)\n",
    "        prev_observation = observation\n",
    "        observation, reward, done, info = model._env.step(action)\n",
    "        \n",
    "        next_ = model.predict(observation)\n",
    "        # print(next_[0])\n",
    "        G = reward + gamma*np.max(next_[0])\n",
    "        model.update(prev_observation, action, G)\n",
    "        \n",
    "        total_reward += reward\n",
    "        iters += 1\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_avg(total_rewards):\n",
    "    N = len(total_rewards)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = total_rewards[max(0,t-100):(t+1)].mean()\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(\"Running Average\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "featT = FeatureTransformer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmnasr/python-venvs/opencv_python3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = Model(env=env, feature_transformer=featT, learning_rate=\"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmnasr/python-venvs/opencv_python3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:9 eps:0.07602310586545652 total_reward:-200.0.\n",
      "t:19 eps:0.05606127246667495 total_reward:-200.0.\n",
      "t:29 eps:0.041340934901356444 total_reward:-200.0.\n",
      "t:39 eps:0.03048580282465283 total_reward:-161.0.\n",
      "t:49 eps:0.02248096653066055 total_reward:-127.0.\n",
      "t:59 eps:0.016578007115626455 total_reward:-200.0.\n",
      "t:69 eps:0.012225022422898742 total_reward:-157.0.\n",
      "t:79 eps:0.009015026486477024 total_reward:-150.0.\n",
      "t:89 eps:0.00664789803572497 total_reward:-146.0.\n",
      "t:99 eps:0.0049023204046809935 total_reward:-116.0.\n",
      "t:109 eps:0.003615089344181072 total_reward:-143.0.\n",
      "t:119 eps:0.002665854103279885 total_reward:-112.0.\n",
      "t:129 eps:0.0019658651345404307 total_reward:-148.0.\n",
      "t:139 eps:0.0014496763804316573 total_reward:-106.0.\n"
     ]
    }
   ],
   "source": [
    "N_episode = 300\n",
    "total_rewards = np.empty(N_episode)\n",
    "for t in range(N_episode):\n",
    "    eps = 0.1*(0.97**t)\n",
    "    tot_reward = play_one_episode(model, eps=eps, gamma=0.99)\n",
    "    total_rewards[t] = tot_reward\n",
    "    if (t+1) % 10 == 0:\n",
    "        print(\"t:{} eps:{} total_reward:{}.\".format(t, eps, tot_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_running_avg(total_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
